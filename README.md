# ğŸŒ¦ï¸ Pipeline MÃ©tÃ©o -- Landing â†’ Staging â†’ Refined â†’ Warehouse

Pipeline locale inspirÃ©e d'une architecture Data AWS (S3 / Glue /
Athena).

## ğŸ§­ Objectif

Construire une mini-pipeline Data complÃ¨te avec :

-   **Landing** â†’ donnÃ©es brutes (CSV)
-   **Staging** â†’ donnÃ©es nettoyÃ©es (JSON Lines)
-   **Refined** â†’ format optimisÃ© (Parquet + Snappy)
-   **Warehouse** â†’ base analytique locale (DuckDB)
-   **Notebook** â†’ analyse exploratoire et visualisation

> Cette structure imite un vrai **Data Lakehouse**.

------------------------------------------------------------------------

## ğŸš€ Installation

### 1) CrÃ©er un environnement virtuel

``` bash
python -m venv .venv
source .venv/Scripts/activate
```

### 2) Installer les dÃ©pendances

``` bash
pip install --upgrade pip
pip install -r requirements.txt
```

------------------------------------------------------------------------

## ğŸ—ï¸ Pipeline -- Ã‰tapes

### 1ï¸âƒ£ GÃ©nÃ©ration des donnÃ©es mÃ©tÃ©o (Landing - CSV)

``` bash
python scripts/generate_weather_data.py
```

### 2ï¸âƒ£ Nettoyage des donnÃ©es (Staging - JSON)

``` bash
python scripts/clean_weather_data.py
```

### 3ï¸âƒ£ Conversion en Parquet (Refined)

``` bash
python scripts/to_parquet.py
```

### 4ï¸âƒ£ Chargement dans DuckDB (Warehouse)

``` bash
python scripts/load_to_db.py
```

### 5ï¸âƒ£ Analyse dans le Notebook

``` bash
jupyter notebook analysis/meteo_analysis.ipynb
```

------------------------------------------------------------------------

## âš¡ Pipeline complÃ¨te automatique (1 seule commande)

``` bash
python scripts/run_pipeline.py
```

------------------------------------------------------------------------

## ğŸ“‚ Structure du projet

    pipeline_meteo/
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ raw/        # Landing
    â”‚   â”œâ”€â”€ staging/    # Nettoyage
    â”‚   â””â”€â”€ refined/    # Parquet optimisÃ©
    â”‚
    â”œâ”€â”€ warehouse/
    â”‚   â””â”€â”€ weather.duckdb   # EntrepÃ´t analytique
    â”‚
    â”œâ”€â”€ analysis/
    â”‚   â””â”€â”€ meteo_analysis.ipynb
    â”‚
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ generate_weather_data.py
    â”‚   â”œâ”€â”€ clean_weather_data.py
    â”‚   â”œâ”€â”€ to_parquet.py
    â”‚   â”œâ”€â”€ load_to_db.py
    â”‚   â””â”€â”€ run_pipeline.py      # Orchestration du pipeline complet
    â”‚
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ README.md

------------------------------------------------------------------------

## ğŸ“Š Pipeline (Diagramme Mermaid)

``` mermaid
graph LR
    A[CSV - Landing] --> B[JSON - Staging]
    B --> C[Parquet - Refined]
    C --> D[DuckDB - Warehouse]
    D --> E[Jupyter Notebook - Analyse]
```

------------------------------------------------------------------------

## ğŸ› ï¸ Technologies utilisÃ©es

  Zone        Technologie
  ----------- -----------------------------
  Landing     CSV
  Staging     JSON Lines
  Refined     Parquet (Snappy)
  Warehouse   DuckDB
  Analyse     Pandas, Matplotlib, Seaborn
  Scripts     Python

------------------------------------------------------------------------

## ğŸ“Œ Notes

-   Le pipeline est totalement reproductible.
-   Peut Ãªtre Ã©tendu vers **AWS (S3, Glue, Athena)**.
-   DuckDB imite un moteur SQL analytique type **Athena**.
