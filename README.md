# ğŸŒ¦ï¸ Pipeline MÃ©tÃ©o â€” API â†’ Landing â†’ Staging â†’ Refined â†’ Warehouse

Pipeline locale inspirÃ©e dâ€™une architecture Data AWS (S3 / Glue / Athena),  
avec ingestion **rÃ©elle** des donnÃ©es mÃ©tÃ©o via API Openâ€‘Meteo.

## ğŸ§­ Objectif

Construire une miniâ€‘pipeline Data complÃ¨te :

- **Ingestion API** â†’ CSV dans **S3 Landing**
- **Staging** â†’ nettoyage â†’ JSON Lines dans **S3 Staging**
- **Refined** â†’ conversion Parquet + compression Snappy dans **S3 Refined**
- **Warehouse** â†’ chargement dans **DuckDB** en local
- **Notebook** â†’ analyse exploratoire & visualisation

Cette structure reproduit les concepts dâ€™un **Data Lakehouse** moderne.

---

## ğŸš€ Installation

### 1) CrÃ©er un environnement virtuel

```bash
python -m venv .venv
source .venv/Scripts/activate  # Windows
```

### 2) Installer les dÃ©pendances

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 3) Configuration AWS

Le pipeline utilise automatiquement les credentials AWS configurÃ©s via :

```bash
aws configure
```

Les buckets S3 utilisÃ©s sont :

- `weather-pipeline-landing-vincent`
- `weather-pipeline-staging-vincent`
- `weather-pipeline-refined-vincent`

---

## ğŸ—ï¸ Pipeline â€” Ã‰tapes

### 1ï¸âƒ£ Ingestion mÃ©tÃ©o via API â†’ **S3 Landing**

```bash
python scripts/ingest_weather_api.py
```

### 2ï¸âƒ£ Nettoyage â†’ **S3 Staging**

```bash
python scripts/clean_weather_data.py
```

### 3ï¸âƒ£ Conversion Parquet â†’ **S3 Refined**

```bash
python scripts/to_parquet.py
```

### 4ï¸âƒ£ Chargement dans DuckDB (Warehouse local)

```bash
python scripts/load_to_db.py
```

### 5ï¸âƒ£ Analyse Notebook

```bash
jupyter notebook analysis/meteo_analysis.ipynb
```

---

## âš¡ Pipeline complÃ¨te en une seule commande

```bash
python scripts/run_pipeline.py
```

---

## ğŸ“‚ Structure du projet

```
pipeline_meteo/
â”œâ”€â”€ warehouse/
â”‚   â””â”€â”€ weather.duckdb        # EntrepÃ´t analytique local
â”‚
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ meteo_analysis.ipynb
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_weather_api.py   # API â†’ Landing (S3)
â”‚   â”œâ”€â”€ clean_weather_data.py   # Landing â†’ Staging (S3)
â”‚   â”œâ”€â”€ to_parquet.py           # Staging â†’ Refined (S3)
â”‚   â”œâ”€â”€ load_to_db.py           # Refined (S3) â†’ DuckDB
â”‚   â””â”€â”€ run_pipeline.py         # Orchestration complÃ¨te
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

Les dossiers `data/raw`, `data/staging` et `data/refined` existent uniquement pour compatibilitÃ©,  
mais **les fichiers sont dÃ©sormais stockÃ©s dans S3**, pas en local.

---

## ğŸ“Š SchÃ©ma du pipeline (Mermaid)

```mermaid
graph LR
    A[API Openâ€‘Meteo] --> B[S3 Landing (CSV)]
    B --> C[S3 Staging (JSON Lines)]
    C --> D[S3 Refined (Parquet Snappy)]
    D --> E[DuckDB (Warehouse local)]
    E --> F[Jupyter Notebook Analyse]
```

---

## ğŸ› ï¸ Technologies utilisÃ©es

| Zone        | Technologie |
|-------------|-------------|
| Ingestion   | API Openâ€‘Meteo |
| Landing     | S3 (CSV) |
| Staging     | S3 (JSON Lines) |
| Refined     | S3 (Parquet Snappy) |
| Warehouse   | DuckDB |
| Analyse     | Pandas, Matplotlib, Seaborn |
| Orchestration | Python |

---

## ğŸ“Œ Notes

- Pipeline entiÃ¨rement reproductible.
- PrÃ©vu pour migrer facilement vers **AWS Glue**, **AWS Athena**, **Step Functions**.
- DuckDB est utilisÃ© ici comme moteur analytique local (Ã©quivalent Athena S3).
