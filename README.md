# üå¶Ô∏è Pipeline M√©t√©o ‚Äî API ‚Üí Landing ‚Üí Staging ‚Üí Refined ‚Üí Warehouse

Pipeline locale inspir√©e d‚Äôune architecture Data AWS (S3 / Glue / Athena),
avec ingestion **r√©elle** des donn√©es m√©t√©o via API.

## üß≠ Objectif

Construire une mini-pipeline Data compl√®te avec :

- **Ingestion API** ‚Üí donn√©es brutes (CSV en Landing)
- **Staging** ‚Üí nettoyage + normalisation (JSON Lines)
- **Refined** ‚Üí format optimis√© (Parquet + Snappy)
- **Warehouse** ‚Üí entrep√¥t analytique local (DuckDB)
- **Notebook** ‚Üí analyse exploratoire et visualisation

> L‚Äôobjectif est p√©dagogique : reproduire les concepts d‚Äôun **Data Lakehouse**
> mais en local, avant de migrer vers AWS (S3 ‚Üí Glue ‚Üí Athena).

---

## üöÄ Installation

### 1) Environnement virtuel

```bash
python -m venv .venv
source .venv/Scripts/activate
```

### 2) Installer les d√©pendances

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

---

## üèóÔ∏è Pipeline ‚Äî √âtapes

### 1Ô∏è‚É£ Ingestion m√©t√©o via API (Landing - CSV)

```bash
python scripts/ingest_weather_api.py
```

### 2Ô∏è‚É£ Nettoyage des donn√©es (Staging - JSON)

```bash
python scripts/clean_weather_data.py
```

### 3Ô∏è‚É£ Conversion en Parquet (Refined)

```bash
python scripts/to_parquet.py
```

### 4Ô∏è‚É£ Chargement dans DuckDB (Warehouse)

```bash
python scripts/load_to_db.py
```

### 5Ô∏è‚É£ Analyse dans le Notebook

```bash
jupyter notebook analysis/meteo_analysis.ipynb
```

---

## ‚ö° Pipeline compl√®te en une seule commande

```bash
python scripts/run_pipeline.py
```

---

## üìÇ Structure du projet

```
pipeline_meteo/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/           # Landing (CSV depuis l'API)
‚îÇ   ‚îú‚îÄ‚îÄ staging/       # Donn√©es nettoy√©es (JSON Lines)
‚îÇ   ‚îî‚îÄ‚îÄ refined/       # Parquet optimis√©
‚îÇ
‚îú‚îÄ‚îÄ warehouse/
‚îÇ   ‚îî‚îÄ‚îÄ weather.duckdb   # Entrep√¥t analytique
‚îÇ
‚îú‚îÄ‚îÄ analysis/
‚îÇ   ‚îî‚îÄ‚îÄ meteo_analysis.ipynb
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ ingest_weather_api.py
‚îÇ   ‚îú‚îÄ‚îÄ clean_weather_data.py
‚îÇ   ‚îú‚îÄ‚îÄ to_parquet.py
‚îÇ   ‚îú‚îÄ‚îÄ load_to_db.py
‚îÇ   ‚îî‚îÄ‚îÄ run_pipeline.py
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üìä Pipeline (Diagramme Mermaid)

```mermaid
graph LR
    A[API Weather ‚Üí CSV (Landing)]
        --> B[JSON Lines (Staging)]
    B --> C[Parquet Snappy (Refined)]
    C --> D[DuckDB (Warehouse)]
    D --> E[Jupyter Notebook (Analyse)]
```

---

## üõ†Ô∏è Technologies utilis√©es

| Zone        | Technologie                        |
|-------------|------------------------------------|
| Landing     | API Open-Meteo ‚Üí CSV               |
| Staging     | JSON Lines                         |
| Refined     | Parquet (Snappy)                   |
| Warehouse   | DuckDB                             |
| Analyse     | Pandas, Matplotlib, Seaborn        |
| Scripts     | Python                             |

---

## üìå Notes

- Pipeline totalement reproductible et ex√©cutable en local.
- Con√ßu pour √™tre migr√© vers **AWS (S3, Glue, Athena)**.
- DuckDB simule un moteur analytique type **Athena**.
